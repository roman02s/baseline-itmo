import os
import re

import openai
import requests
from typing import List, Optional

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, HttpUrl, ValidationError
from dotenv import load_dotenv

from crewai.crews import CrewOutput
from crewai import Agent, Task, Crew, Process
from crewai_tools import ScrapeWebsiteTool, SerperDevTool
from langchain_openai import ChatOpenAI

# üîÑ Load environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SERPER_API_KEY = os.getenv("SERPER_API_KEY")

if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY not found in environment variables.")
if not SERPER_API_KEY:
    raise ValueError("SERPER_API_KEY not found in environment variables.")

openai.api_key = OPENAI_API_KEY

# üìå Initialize tools
search_tool = SerperDevTool()
scrape_tool = ScrapeWebsiteTool()

# üöÄ Initialize FastAPI
app = FastAPI()


# üìå Define Data Models
class PredictionRequest(BaseModel):
    id: int
    query: str


class PredictionResponse(BaseModel):
    id: int
    answer: Optional[int]
    reasoning: str
    sources: List[HttpUrl]


# üîç **Search News Agent**
search_agent = Agent(
    role="Search Agent",
    backstory="–ê–≥–µ–Ω—Ç, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –∑–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –≤–æ–ø—Ä–æ—Å—É {query}, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ò–¢–ú–û.",
    goal="–ò—Å–∫–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏, –∫–∞—Å–∞—é—â–∏–µ—Å—è –ò–¢–ú–û –ø–æ –≤–æ–ø—Ä–æ—Å—É {query} —Å –ø–æ–º–æ—â—å—é SerperDevTool.",
    tools=[search_tool],
    allow_delegation=False,
    verbose=True,
    max_iter=1,
    max_rpm=10,
    llm=ChatOpenAI(model_name="gpt-4o", temperature=0.7)
)

# üìÑ **Scrape News Agent**
scrape_agent = Agent(
    role="Scraping Agent",
    backstory="–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–ª—é—á–µ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–æ–ø—Ä–æ—Å—É {query} –∏–∑ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü.",
    goal="–ò–∑–≤–ª–µ–∫–∞—Ç—å –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–æ–ø—Ä–æ—Å—É {query} –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ–± –ò–¢–ú–û.",
    tools=[scrape_tool],
    allow_delegation=False,
    verbose=True,
    max_iter=1,
    max_rpm=10,
    llm=ChatOpenAI(model_name="gpt-4o", temperature=0.7)
)

# ü§ñ **Answer Processing Agent**
answer_agent = Agent(
    role="Answer Agent",
    backstory="–ê–≥–µ–Ω—Ç, –æ—Ç–≤–µ—á–∞—é—â–∏–π –Ω–∞ –≤–æ–ø—Ä–æ—Å {query} –∏—Å–ø–æ–ª—å–∑—É—è –∏–∑–≤–ª–µ—á—ë–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.",
    goal="–ù–∞–ø–∏—Å–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å {query}.",
    tools=[],
    allow_delegation=False,
    verbose=True,
    max_iter=1,
    max_rpm=10,
    llm=ChatOpenAI(model_name="gpt-4o", temperature=0.7)
)

# üîé **Task 1: Search ITMO News**
search_task = Task(
    description="–ù–∞–π–¥–∏ 2 —Å–∞–π—Ç–∞ –ø–æ –≤–æ–ø—Ä–æ—Å—É {query} –ø—Ä–æ –ò–¢–ú–û –∏—Å–ø–æ–ª—å–∑—É—è SerperDevTool.",
    expected_output="–°–ø–∏—Å–æ–∫ –∏–∑ 2 URL-–∞–¥—Ä–µ—Å–æ–≤ –ø–æ –≤–æ–ø—Ä–æ—Å—É {query} –æ–± —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ –ò–¢–ú–û.",
    agent=search_agent
)

# üìë **Task 2: Scrape News Articles**
scrape_task = Task(
    description="–ò–∑–≤–ª–µ–∫–∏ –û–î–ù–û –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–∑ —Å–∞–π—Ç–∞ –ø–æ –≤–æ–ø—Ä–æ—Å—É {query}, –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –Ω–∞ —ç—Ç–∞–ø–µ –ø–æ–∏—Å–∫–∞.",
    expected_output="–û–î–ù–û –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –æ—Ç–≤–µ—Ç–æ–º –Ω–∞ –≤–æ–ø—Ä–æ—Å {query} –∏–∑ —Å—Ç–∞—Ç—å–∏.",
    agent=scrape_agent,
    context=[search_task]
)

# ü§ñ **Task 3: Process Question and Answer**
answer_task = Task(
    description="–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å {query} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.",
    expected_output="–û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–æ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å {query}, –ø–æ—è—Å–Ω–∏–≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç.",
    agent=answer_agent,
    context=[scrape_task]
)

# üöÄ **Define Crew**
crew = Crew(
    agents=[search_agent, scrape_agent, answer_agent],
    tasks=[search_task, scrape_task, answer_task],
    process=Process.sequential,
    verbose=True
)


# üéØ **Extract Multiple Choice Options**
def extract_answer_options(query: str) -> List[str]:
    options = []
    for num in range(1, 11):
        split_query = query.split(f"{num}. ")
        if len(split_query) > 1:
            options.append(split_query[1].split("\n")[0])
    return options


# üî• **Find Correct Answer from GPT Response**
def find_correct_answer(gpt_response: str, answer_options: List[str]) -> Optional[int]:
    for i, option in enumerate(answer_options, 1):
        # Simple substring match ‚Äì adjust if needed
        if option.lower() in gpt_response.lower():
            return i
    return None


def extract_urls_from_text(text: str) -> List[HttpUrl]:
    """Simple regex-based URL extraction from text."""
    pattern = r'(https?://[^\s)]+)'
    found_urls = re.findall(pattern, text)
    valid_urls = []
    for url in found_urls:
        url = url.rstrip(').,;')
        try:
            valid_urls.append(url)
        except ValidationError:
            continue
    return valid_urls


def chunk_text(text: str, max_chars: int = 4000) -> list:
    """Split text into chunks of approximately `max_chars` characters."""
    chunks = []
    start = 0
    while start < len(text):
        end = start + max_chars
        chunks.append(text[start:end])
        start = end
    return chunks


def summarize_chunk(chunk: str, model="gpt-3.5-turbo") -> str:
    """Summarize a single chunk using a smaller or cheaper model (e.g., GPT-3.5)."""
    prompt = f"Summarize the following text as concisely as possible:\n\n{chunk}"
    response = openai.ChatCompletion.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )
    return response["choices"][0]["message"]["content"].strip()


def summarize_large_text(full_text: str) -> str:
    """
    1. Split the text into manageable chunks.
    2. Summarize each chunk.
    3. Combine those chunk-summaries into a final summary.
    """
    # 1) Split the text into chunks
    chunks = chunk_text(full_text, max_chars=3000)  # adjust as needed

    # 2) Summarize each chunk individually
    partial_summaries = []
    for chunk in chunks:
        summary = summarize_chunk(chunk)
        partial_summaries.append(summary)

    # 3) Merge partial summaries into one text
    merged_text = "\n\n".join(partial_summaries)

    # 4) Summarize the merged text again if needed
    if len(merged_text) > 3000:  # or any threshold you want
        return summarize_chunk(merged_text)
    else:
        return merged_text


# --------------------- API ENDPOINT --------------------- #
@app.post("/api/request", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """
    Kick off the entire Crew. The search_task, scrape_task, and answer_task
    will run in sequence. We then parse the final outputs to fill the response.
    """
    try:
        # 1) Kick off the entire crew with the user's query as input
        crew_output: CrewOutput = crew.kickoff(inputs={"query": request.query})

        # 2) The CrewOutput has a tasks_output list, matching the order: [search_task, scrape_task, answer_task].
        tasks_out = crew_output.tasks_output

        if len(tasks_out) < 3:
            raise ValueError("Not enough tasks output. Expected 3 tasks in tasks_output.")

        # 3) Get the raw text from each step
        #    tasks_out[0] => search_task
        #    tasks_out[1] => scrape_task
        #    tasks_out[2] => answer_task (the final)
        search_text = tasks_out[0].raw
        scrape_text = tasks_out[1].raw
        final_text = tasks_out[2].raw

        # 4) Parse actual sources from the search step
        sources = extract_urls_from_text(search_text)

        # 5) Derive the final short answer from final_text
        answer_options = extract_answer_options(request.query)
        answer = find_correct_answer(final_text, answer_options)

        # 6) Return the final result
        return PredictionResponse(
            id=request.id,
            answer=answer,
            reasoning=final_text.strip(),
            sources=sources  # from the search step
        )

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"
        )
