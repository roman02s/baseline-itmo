import os
import time
import openai
import requests
from typing import List, Optional

from lxml import html

from fastapi import FastAPI, HTTPException, Request, Response
from pydantic import BaseModel, HttpUrl
from dotenv import load_dotenv
from crewai_tools import ScrapeWebsiteTool, SerperDevTool

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SERPER_API_KEY = os.getenv("SERPER_API_KEY")

if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY not found in environment variables.")
if not SERPER_API_KEY:
    raise ValueError("SERPER_API_KEY not found in environment variables.")

openai.api_key = OPENAI_API_KEY

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
search_tool = SerperDevTool()
scrape_tool = ScrapeWebsiteTool()


# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ö–µ–º –¥–∞–Ω–Ω—ã—Ö
class PredictionRequest(BaseModel):
    id: int
    query: str


class PredictionResponse(BaseModel):
    id: int
    answer: Optional[int]
    reasoning: str
    sources: List[HttpUrl]


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI
app = FastAPI()


# üîç **Search news using Google (SerperDevTool)**
def search_itmo_news(query: str) -> List[str]:
    """Searches for ITMO news articles using SerperDevTool (Google search restricted to news.itmo.ru)."""
    search_query = f"site:news.itmo.ru {query}"
    results = search_tool.run(search_query)

    if not results or "organic" not in results:
        return []

    # Extract up to 10 links from search results
    links = [res["link"] for res in results.get("organic", [])[:10]]
    return links


# üìÑ **Scrape article content**
def scrape_news_page(url: str) -> dict:
    """Extracts news article content using ScrapeWebsiteTool."""
    scraped_content = scrape_tool.run(url)

    return {
        "content": scraped_content if scraped_content else "No content available",
        "url": url
    }


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫
async def search_links(query: str) -> List[HttpUrl]:
    search_url = "https://api.duckduckgo.com/"
    params = {
        "q": query,
        "format": "json",
        "no_html": 1,
        "skip_disambig": 1
    }
    response = requests.get(search_url, params=params)
    results = response.json().get("RelatedTopics", [])

    links = []
    for result in results[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 3 —Å—Å—ã–ª–∫–∞–º–∏
        if "FirstURL" in result:
            links.append(result["FirstURL"])

    return [HttpUrl(link) for link in links if link]


# –§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞
def extract_answer_options(query: str) -> List[str]:
    """–†–∞–∑–¥–µ–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–∞ –ø–æ —à–∞–±–ª–æ–Ω—É '1. ', '2. ' –∏ —Ç.–¥."""
    options = []
    for num in range(1, 11):  # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–æ 10 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
        split_query = query.split(f"{num}. ")
        if len(split_query) > 1:
            options.append(split_query[1].split("\n")[0])  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –≤–∞—Ä–∏–∞–Ω—Ç–∞
    return options


# üî• –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
def find_correct_answer(gpt_response: str, answer_options: List[str]) -> Optional[int]:
    """–°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç GPT —Å –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–∞ –∏ –Ω–∞—Ö–æ–¥–∏–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –Ω–æ–º–µ—Ä."""
    for i, option in enumerate(answer_options, 1):
        if option.lower() in gpt_response.lower():
            return i  # –ù–æ–º–µ—Ä –≤–∞—Ä–∏–∞–Ω—Ç–∞
    return None


# –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞
@app.post("/api/request", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    try:
        from openai import OpenAI

        client = OpenAI(api_key=OPENAI_API_KEY)

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": request.query}],
            max_tokens=200
        )

        gpt_response = response.choices[0].message.content.strip()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≤–æ–ø—Ä–æ—Å —Å –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤
        answer_options = extract_answer_options(request.query)

        # –ò—â–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
        answer = find_correct_answer(gpt_response, answer_options)

        # Search ITMO news
        news_links = search_itmo_news(request.query)

        # Scrape first 3 news articles
        scraped_news = [scrape_news_page(url) for url in news_links[:3]]

        # Collect sources
        sources = [news["url"] for news in scraped_news if news]

        return PredictionResponse(
            id=request.id,
            answer=answer,
            reasoning=gpt_response,
            sources=sources
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
